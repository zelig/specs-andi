
Pushsync protocol attempts to push a chunk to its destination
neighborhood by selecting a peer (or peers) that is closer to it and
further delegating the act of chunk transfer.

Every chunk is sure to have a destination neighborhood and is detected
by comparing the overlay address of the peer and the chunk address. This
operation returns the PO (proximity order). If this PO is greater or
equal to the value of the storage radius then the chunk has reached its
neighborhood.

That means that a chunk will travel through the network until it lands
on a peer whose PO (between peer address and the chunk address) is
greater or equal to the storage radius of that peer. The peer has the
responsibility to store it in its reserve and sign a receipt with the
peers signature and on the origin node it will confirm that proximity
order between the storer node and chunk address is within the storage
radius. If it's outside of the radius - we have a shallow receipts - and
the chunk will be re-pushed eventually.

A chunk will be pushed multiple time if it's associated with more than 
one postage stamp. The receiving side is responsible to take this into 
account and persist the chunk accordingly.

Pushsync protocol defines the following streams:
\begin{itemize}
    \item ID: \texttt{/swarm/pushsync/1.3.0/pushsync}
    \item Serialization: Varint delimited Protobuf
\end{itemize}

Message definitions:
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{syntax = }\StringTok{"proto3"}\NormalTok{;}

\KeywordTok{package}\NormalTok{ pushsync;}

\KeywordTok{message}\NormalTok{ Delivery \{}
  \DataTypeTok{bytes}\NormalTok{ Address = }\DecValTok{1}\NormalTok{;}
  \DataTypeTok{bytes}\NormalTok{ Data = }\DecValTok{2}\NormalTok{;}
  \DataTypeTok{bytes}\NormalTok{ Stamp = }\DecValTok{3}\NormalTok{;}
\NormalTok{\}}

\KeywordTok{message}\NormalTok{ Receipt \{}
  \DataTypeTok{bytes}\NormalTok{ Address = }\DecValTok{1}\NormalTok{;}
  \DataTypeTok{bytes}\NormalTok{ Signature = }\DecValTok{2}\NormalTok{;}
  \DataTypeTok{bytes}\NormalTok{ Nonce = }\DecValTok{3}\NormalTok{;}
  \DataTypeTok{string}\NormalTok{ Err = }\DecValTok{4}\NormalTok{;}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

There are three roles that a peer can 'play': originator, forwarder and storer:
\begin{itemize}
\tightlist
\item originator is the node that is the first one to `see' a new chunk
\item forwarder is the node that takes the chunk and moves it closer to the storer
\item storer is the node that has the responsibility to persist the chunk and to forward it to its neighborhood peers
\end{itemize}

\subsubsection{Pushing side}\label{pushing-side}

An originator or an forwarder will create a new stream to which it will
write a \texttt{Delivery} message containing the chunk data with its
associated stamp. It will then wait for the response message. If the
reply contains a non empty error, the pushing side will attempt pushing
the chunk to the next best peer, as described bellow.

\subsubsection{Storer side}\label{storer-side}

Once the upstream peer receives the \texttt{Delivery} message and
concludes that it is responsible to store the chunk it will perform the
appropriate persistence interactions and will return a \texttt{Receipt}
message. After the response is sent, this node should wait for
requesting node to close it's side of the stream before closing the
streaming and moving on.

\subsubsection{Retries and error handling}\label{retries-and-error-handling}

If the node that pushed the chunk receives an error from the upstream it
might retry the action multiple time if it's an origin node. If it
exhaust its attempts to push the chunk to the closest peer it will move
on to the next closest peer candidate and repeat the request. If the
failure reason is that the pushing peer is in overdraft, it will
de-prioritize the corresponding upstream, allowing for balance
replenishment. A `forwarder' will give up after the first failure while
an `origin' node might repeat the request multiple time towards the same
peer before giving up. A `multiplexer' might attempt to push the chunk
more than one time since being in the close proximity to the
neighborhood it has the confidence that the nodes in its reach are the
ones responsible for persisting the chunk.

\subsubsection{Further details}\label{further-details}

As a forwarder when we receive a chunk - if we're in reachable state and
within the storage radius - we store the chunk to disk - and then we
push the chunk to closest known peer.

If we are an origin peer we should not store the chunk initially so that
the chunk is always forwarded into the network.

If no peer can be found from an origin peer, the origin peer may store
the chunk.

Non-origin peers store the chunk if the chunk is within depth.

For non-origin peers, if the chunk is not within depth, they may store
the chunk if they are the closest peer to the chunk.

In determining the closest peer we compare the proximity of the given
peer with the chunk address. In order to determine if we act as a
multiplexer and push the chunk in parallel to multiple peers we then
compare the proximity value with the storage radius. We sent out the
chunk to the neighborhoods in parallel (to maximum \texttt{2} peers at a
time).

After pushing a chunk we await for reply containing a receipt. If the
response comes back with an error we re-try the attempt. Once we exhaust
the retries we return from the function with a \texttt{ErrNoPush} and
cancel the context, thus stopping all ongoing go-routines.
\begin{itemize}
\tightlist
\item If that is true then the skipping of peers and selection of peers needs to be documented:
\item `choice strategy' - how kademlia table is under specifying as there's several nodes in the same proximity bin and you will need a secondary choice between them or some sort of multiplexing for selecting the optimal peer/s and it's not well documented:
\end{itemize}

The Kademlia component has a priority list on how to choose the closest
peer:
\begin{itemize}
\tightlist
\item first it will decide if to include self (only full nodes who are not the origin node)
\item lookup among peers who are reachable and healthy
\item lookup among peers who are reachable
\item lookup among all others
\end{itemize}

The default strategy for any downstream error is retry with a delay that
is variable depending on how many `in-flight' actions we have at any
given moment.

Each time a chunk is being pushed to a peer - its (peer) address is
added to skipList to avoid re-trying the same chunk/peer pair. The
timeout is 5 min until the next retry. The peer address is added to the
skiplist even if the attempt was successful - this is because the origin
can notice that the receipt is too shallow, so the pusher can consider
the attempt as unsuccessful.

Receiving an invalid chunk in the response will result in blocklisting
of the peer that provided the chunk.

%\section{ \statusorange}## Pushsync

%Pushsync protocol is responsible for ensuring delivery of the chunk to its prescribed storer after it has been uploaded to any arbitrary node.

%It works in a similar way to the Retrieval protocol in the sense that the chunk is being passed to a peer whose address is closest to the chunk address and a custody receipt is received in response.

%Then the same process is repeated until the chunk eventually reaches the storer node located in a certain "neighborhood".

%Since the Pushsync protocol is a "mirror" version of the Retrieval protocol - it ensures that a successfully uploaded chunk is retrievable from the same "neighborhood" by the virtue of the fact that nodes in a neighborhood are connected to each other.

%\subsection{ \statusorange}### Multiplexing

%Multiplexing is a recommended node strategy for the push sync protocol that involves early replication and opportunistic receipting. Its intention is to reduce the dependence on single closest nodes and to improve on network performance, i.e., push sync success rate, bandwidth overhead and latency.

%\subsection{ \statusorange}#### Context

%The current implementation of the push sync protocol aims to push a chunk to the closest node in the neigbhourhood which is then supposed to give out a receipt.

%This is motivated by the  retrieval protocol that aims to find the chunk at this closest node.

%When the closest node hands out a receipt, this node also replicates the chunk to 3 peers in the neigbourhood which are further away from the chunk than him.

%This replication takes place to ensure that the chunk is not lost when the closest node shuts down before the chunk is not pull-sync'ed and to speed up the spreading of the chunk in the neigbhourhood, in advance of pull sync.

%\subsection{ \statusorange}#### Problem

%Treating the closest node as a single target of push sync is fragile. If this peer has a malperforming blockchain backend, slow or incomplete connectivity or is malicious, it may not spread the chunk and/or does not respond with a receipt.

%In this case, currently, the originator must retry the entire push-sync operation many times before the other peers within neigbhorhood recognise the improper behavior.

%In-neigbhorhood retries are ideally avoided because such retries might cause the downstream timeouts to expire.

%In case of incomplete connectivity, the push sync protocol can end at a different branch of the neigbhorhood than the retrieval protocol--causing the chunk not to be retrievable.

%It should be noted that the pull sync protocol (may?) remedies this problem with a small time-delay.

%\subsection{ \statusorange}#### Multiplexing: early replication within neighbourhood

%The first node in the push sync forward chain that falls within the neigbhorhood acts as *multiplexer*, i.e., it forwards the request to a number of closest nodes and responds with a self-signed receipt.

%Thus in achieving retrievability and security via early replication, we do not critically rely on the closest node to be available anymore.

%\subsection{ \statusorange}#### Push sync flow

%We define the different roles peers have as part of the push sync forwarding chain:

%- originator -- creator of the request
%- forwarder -- closer to the chunk than the originator, further away away than the 1-before node.
%- multiplexer -- first node in the forward chain who is in the neigbhorhood
%- closest nodes -- according to the downstream node (usually the multiplexer), within the `n` closest nodes to the chunks (not including self)

%we describe the envisioned flow of push sync by describing the intended behaviour strategy of the various roles.

%1. originator sends chunk to a peer closer to the chunk.
%2. forwarder(s) forwards chunk that ends up with a node already within the neighbourhood that acts as multiplexer
%3. The multiplexer concurrently sends the chunk to 3 closest nodes attaching a multiplexing-list as part of the protocol message. At the same time they respond to their upstream peer with a self-signed receipt (unless the multiplexer is itself the originator).
%4. Non-multiplexing closest nodes, i.e., nodes in the neighbourhood that receive the pushsync message from a not-closest neighbour with a multiplexing list included, validate whether, based on their view, the multiplexing list covers all 3 closest nodes (potentially including the peer and/or the upstream peer themselves). If not, the node forwards the chunk to the peers left out. These peers are also added to the multiplexing list received from upstream and the extended list is attached with the chunk pushed.

%If the multiplexer node does not know a closest peer *p* but several of its chosen closest nodes do, then that node *p* will receive the same pushsynced chunk multiple times

%\subsection{ \statusorange}### Appendix

%The protobuf definitions

%\begin{definition}[Push-sync protocol messages]\label{def:push-sync-messages}

%\begin{lstlisting}[]
%// /swarm/push-sync/1.0.0/peers
%// Copyright 2020 The Swarm Authors. All rights reserved.
%// Use of this source code is governed by a BSD-style
%// license that can be found in the LICENSE file.

%syntax = "proto3";

%package pushsync;

%option go_package = "pb";

%message Delivery {
%  bytes Address = 1;
%  bytes Data = 2;
%  bytes Stamp = 3;
%}

%message Receipt {
%  bytes Address = 1;
%  bytes Signature = 2;
%  bytes Nonce = 3;
%}
%\end{lstlisting}
%\end{definition}
