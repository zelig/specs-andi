\chapter{Batch utilisation}\label{sec:appendix-batch-utilisation}


When pushing content to the Swarm network, uploaders are required to
attach an attestation of storage rent prepayment to each chunk they
post. The latter is essentially a wallet registered on the postage
contract seeded with a balance from which storage rent is deduced by the
network-wide incentive system. Since this is reminiscent of buying a
batch of postage stamps and attaching one to each envelope to be posted,
the attestations are called \emph{postage stamps} and the registered
wallet a \emph{postage batch.} One can think of batches as collections
of \emph{storage slots.} The size of a batch is the number of storage
slots and is always specified as a power of 2, with the exponent
called \emph{batch depth.} Each slot can hold at most one chunk.
Putting a chunk into a slot is like issuing a postage stamp.

In practice, the attached postage stamps are digital signatures which
associate the address of a chunk with a \emph{storage slot reference.}
This in turn is composed of 1) a reference to the wallet through its
\emph{batch ID,} and 2) a \emph{within-batch index.} The fact that each
slot can hold at most one chunk ensures that batches cannot issue more
stamps than the volume registered with them. However, for an
\emph{overissuance} incident to be detectable locally by storer nodes,
the within-batch indices are arranged such that the highest $n$ bits
match the prefix of the chunk they are assigned to. These $n$ bits
define $2^n$ \emph{buckets}, the other half of the index is essentially a counter within the buckets, which is
sequentially assigned to chunks. If
the batch depth is $d$ and there
are $2^n$ buckets, then each bucket will hold a maximum of $2^{d-n}$
chunks. $k=d-n$, the log size of a bucket is called
\emph{bucket depth}. The bucket size ($2^k$) provides an exclusive upper bound to
within-bucket indices. This enforces a uniformity of stamp issuance across the $2^n$ buckets, therefore $n$ is called \emph{uniformity depth}.

Overusing a batch is now easily detected by any storer node as long as
their storage depth is shallower than the batch's uniformity depth. In
that case, each bucket of a batch is entirely within the node's reserve.
Overissuance is therefore immediately caught, since the multiple chunks 
assigned to the same slot index are seen by any node in that
neighbourhood.

When all slots are filled, we say the batch is \emph{fully utilised.} 
Although the \emph{a priori} distribution of chunks is uniform (and
therefore the expected number of chunks falling into each bucket is the
same), their stochastic assignment means that there is necessarily some variance. 
It is practically
impossible to fully fill all buckets before eventually attempting to add to one that is already full. 
We assume that the uploader is unable to
affect the address of the chunks (unencrypted fixed content) so in this scenario they are unable to
continue uploading. Because of this, one may legitimately consider the
batch to be no longer usable. The number of stamps hitherto issued by
the batch is called its \emph{effective batch size,} and its ratio to
the batch size its \emph{batch utilization rate.}

Below we explore the effect of batch parameters on their utilization
rate. With an insight into utilization rates as a function of the number
of buckets $2^n$ and the size of buckets $2^k$, we will have a way
to calibrate the expected effective batch size to be presented to users
in the context of a batch purchase user experience.

The problem of assigning chunks to storage slots is analogous with the
process of throwing marbles, one after the other, in boxes which are
initially empty. Each throw may end up in any of the boxes with equal
probability, and thus the marbles get distributed across the boxes more
or less evenly through time. However, the time will come when the boxes
start filling up. At that point, a marble may by chance end up being
thrown into a box that is already full, and thus get rejected.
Substituting chunks for marbles, buckets for boxes, and the act of
signing a stamp for throwing a marble, we recover our original scenario.
Marbles ending up in a box with equal probability corresponds to the
fact that a random chunk has equal chance of being assigned to a bucket
since the hash function has uniform distribution. Repeated rounds of
marble throwing correspond to consecutive stamping of multiple chunks of
the uploaded content; rounds constitute repeated independent trials.

Taking a particular bucket, each round of stamping is a ``success'' if
the stamp falls into the bucket, and a ``failure'' otherwise. Due to the
fact that a marble may end up in each box with equal chance, the
probability of success is $1/n$ and the probability of failure is
$1 - 1/n$. The number of stamps issued to the bucket after a given
number of rounds is described by the
\href{https://en.wikipedia.org/wiki/Negative_binomial_distribution\#Distribution_of_a_sum_of_geometrically_distributed_random_variables}{negative
binomial distribution} $\mathcal{B}(k, 1/n)$, where the first
parameter $k$ is the number of failed rounds before we stop counting,
and the second parameter is the probability of success.

The negative binomial distribution $\mathcal{B}(k, 1/n)$ is equivalent
to the sum of $k$ independent geometrically distributed variables with
parameter $1/n$. Unless $k$ is very small, the central limit theorem
ensures that this sum converges to a normal distribution. Formally, if
$\mathcal{Y}_i(1/n)$ are independent geometrically distributed
variables with parameter $1/n$, and $\mathcal{N}(\mu, \sigma^2)$ is
the normal distribution with mean $\mu$ and variance $\sigma^2$,
then for large $k$ we have
\begin{equation}\protect\hypertarget{eq-binomial-approximation}{}{
\mathcal{B}(k, 1/n) = \sum_{i=1}^k \mathcal{Y}_i(1/n) \approx \mathcal{N}(kn, kn(n-1))
}\label{eq-binomial-approximation}\end{equation} The reason for the
above form of the mean and variance in the normal distribution is as
follows: given a single geometrically distributed variable with
parameter $1/n$, its mean is $n$ and its variance is $n(n-1)$.
When adding these up over $k$ independent variables, we arrive at
$\mu = k n$ and $\sigma^2 = k n (n - 1)$ in the limiting normal
distribution.

The fit of the normal distribution with the negative binomial is robust even for
low values of $k$, as depicted in Figure~\ref{fig-normal-binomial}.

\begin{figure}
  \centering
  \includegraphics[width=0.7\textwidth]{fig/fig-normal-binomial-1.pdf}
  \caption[Approximating negative binomial]{\label{fig-normal-binomial}Approximating negative binomial distributions with limiting normal ones. The number of buckets $n$ is fixed at 100.}
\end{figure}

The negative binomial distribution estimates the number of rounds a
particular bucket fills up, given its size $k$ and the number of
buckets $n$. The rounds of stamping can be conceived of as parallel
independent attempts at filling up buckets. Stamps issued for failed
rounds still end up in one of the other $n - 1$ buckets, and therefore
the same probability variable counts the total stamps issued by the
batch in all buckets at the time the first one fills up. Thus, we want
to know how the \emph{minimum} of $n$ independent normal variates,
$X = \min_{i \in \{1\ldots n\}} \mathcal{N}_i(\mu, \sigma^2)$, is
distributed.

This so-called \emph{extreme value distribution} would give us the
distribution of the absolute number of total rounds needed until the
first event when one of the buckets fills up. Therefore its mean is
exactly the expected value of the number of rounds until the first
bucket-filling event. Since this is when the batch is considered
effectively used up, the mean of the distribution measures the effective
utilization of the batch. Dividing by $n$ gives the expected number of
stamps per bucket making the rate comparable across any parameter of
bucket size. Further dividing by $k$ in turn gives the \emph{expected
normalized utilization rate}, which is now comparable across all $k$
and $n$ values.

The extreme value distribution for the minimum of $n$ independent
variates drawn from the standard normal distribution (with zero mean and
unit variance) is known. It is the
\href{https://en.wikipedia.org/wiki/Gumbel_distribution}{Gumbel
distribution}, which reads
\begin{equation}\protect\hypertarget{eq-gumbel}{}{
\mathcal{G}(x; \alpha, \beta) = \frac{1}{\beta} \exp\left[
\frac{x + \alpha}{\beta} - \exp\left(\frac{x + \alpha}{\beta}\right)\right] .
}\label{eq-gumbel}\end{equation} Here $x$ is the independent variable
(in our case, the number of rounds of stamping), and $\alpha$ and
$\beta$ are the \emph{location} and \emph{scale} parameters,
respectively.\footnote{The Gumbel distribution is often given using the
  convention that one is looking for the maximum of $n$ normal
  variates, instead of their minimum. One can change between the two by
  simply flipping the sign of $x$.} They are in turn given by
\begin{equation}\protect\hypertarget{eq-loc-scale}{}{
\begin{aligned}
  \alpha &= \rho\left( 1 - \frac{1}{n} \right) ,
  \\
  \beta &= \rho\left( 1 - \frac{\text{e}^{-1}}{n} \right) - \alpha ,
\end{aligned}
}\label{eq-loc-scale}\end{equation} where
$\text{e}^{-1} = \exp(-1) \approx 0.368$, $n$ is the number of
normal variates whose minimum we are looking for, and $\rho(x)$ is the
quantile function of the normal distribution (or the inverse of the
\href{https://en.wikipedia.org/wiki/Error_function}{error function}).
The mean, standard deviation, and quantile function of the Gumbel
distribution are
\begin{equation}\protect\hypertarget{eq-mean-var-quantile}{}{
\begin{aligned}
  \mathbb{E}(\mathcal{G}) &= \alpha + \gamma \beta ,
  \\
  \mathbb{S}(\mathcal{G}) &= \beta \, \frac{\pi}{\sqrt{6}} ,
  \\
  Q(p; \mathcal{G}) &= \beta \, \log(-\log(1 - p)) - \alpha ,
\end{aligned}
}\label{eq-mean-var-quantile}\end{equation} where
$\gamma \approx 0.5772$ is the Euler--Mascheroni constant, and
$0 < p < 1$ in $Q(p; \mathcal{G})$ is a probability quantile.

The normal distribution whose variates' minimum values we are looking
for is not standard, but instead has mean $\mu = k n$ (instead of
zero), and variance $\sigma^2 = k n (n-1)$ (instead of one).
Therefore, the Gumbel distribution needs to be appropriately rescaled.
Denoting this scaled probability distribution by $\mathcal{X}$, we
have \begin{equation}\protect\hypertarget{eq-gumbel-scaled}{}{
  \mathcal{X}(x; \mu, \sigma, \alpha, \beta) =
  \frac{1}{\sigma}\mathcal{G}\left(\frac{x - \mu}{\sigma}; \alpha, \beta\right)
}\label{eq-gumbel-scaled}\end{equation} (the overall factor of
$1 / \sigma$ restores the proper normalization of the scaled
function). Consequently, the mean, standard deviation, and quantile
function should also be rescaled:
\begin{equation}\protect\hypertarget{eq-mean-var-quantile-scaled}{}{
\begin{aligned}
  \mathbb{E}(\mathcal{X}) &= \mu - \sigma (\alpha+\gamma\beta) = kn-\sqrt{kn(n-1)}
  \left[(1-\gamma) \rho\left( 1 - \frac{1}{n} \right) + \gamma \rho\left( 1 -
  \frac{\text{e}^{-1}}{n} \right) \right] , 
  \\
  \mathbb{S}(\mathcal{X}) &=\sigma \beta \frac{\pi}{\sqrt{6}} =
  \sqrt{ k n (n-1)} \left[ \rho\left( 1 - \frac{\text{e}^{-1}}{n} \right) -
  \rho\left( 1 - \frac{1}{n} \right) \right] \frac{\pi}{\sqrt{6}} ,
  \\
  Q(p;\mathcal{X})&= \mu-\sigma\left[\alpha-\beta \,
  \log(-\log(1-p))\right] \\ &= kn-\sqrt{ k n (n-1)}\left[\rho\left( 1 -
  \frac{1}{n} \right)-\rho\left( 1 - \frac{\text{e}^{-1}}{n} \right) 
  \log\left(-\log(1-p)\right)\right] .
\end{aligned}
}\label{eq-mean-var-quantile-scaled}\end{equation} Normalizing the mean
and the quantile function by the product $kn$:
\begin{equation}\protect\hypertarget{eq-mean-quant-div}{}{
\begin{aligned}
  \frac{\mathbb{E}(\mathcal{X})}{k n} &= 1 - \sqrt{\frac{(n-1)}{kn}} \left[ (1-\gamma)
  \rho\left( 1 - \frac{1}{n} \right) + \gamma \rho\left( 1 - \frac{\text{e}^{-1}}{n}
  \right) \right] ,
  \\
  \frac{Q(p; \mathcal{X})}{kn} &= 1 - \sqrt{\frac{(n-1)}{kn}}
  \left[\rho\left( 1 - \frac{1}{n} \right)-\rho\left( 1 - \frac{\text{e}^{-1}}{n}
  \right)  \log\left(-\log(1-p)\right)\right] .
\end{aligned}
}\label{eq-mean-quant-div}\end{equation} Taking into account that $k$
and $n$ must both be powers of two, we can write $k = 2^{\kappa}$
and $n = 2^{\nu}$, where $\kappa$ and $\nu$ are positive integers.
We then have \begin{equation}\protect\hypertarget{eq-mean-div-simp}{}{
  \frac{\mathbb{E}(\mathcal{X})}{k n} =
  \frac{\mathbb{E}(\mathcal{X})}{2^{\kappa + \nu}} =
  1 - \sqrt{\frac{2^\nu-1}{2^{\kappa+\nu}}}\left[ (1-\gamma) \rho\left( 1 -
  \frac{1}{2^{\nu}} \right) + \gamma \rho\left( 1 - \frac{\text{e}^{-1}}{2^{\nu}}
  \right) \right] .
}\label{eq-mean-div-simp}\end{equation}

In order to explore this solution, we consider the average expected
utilization rate as a function of uniformity depth, for various log bucket
sizes (see Figure~\ref{fig-mean}). As long as bucket size is around $2^8$ or greater,
the dependence of the utilization rate on $n$ is milder and milder, and at $2^{16}$, it is virtually a flat line at $100\%$.
In general, we see that for the same bucket size, the miss rate
(one minus the utilization rate) increases with the number of buckets. Using
small bucket sizes (up to $2^{10}$), we get unacceptably high miss
rates even with few buckets.%
%
\footnote{Note that for low $k$ but high
  $n$, the solution is unreliable, predicting a negative number of
  stamps per bucket. The prediction will generally not work well if
  $k$ is too small, because the normal approximation to the negative
  binomial distribution starts to break down.}
%  
On the other hand, for
larger bucket sizes (over $2^{10}$), the miss rate is contained at a
tolerable $<10\%$ (on the right in Figure~\ref{fig-mean}).

On the right, we plot the miss rate on a logarithmic scale as a function of log bucket size, for various numbers of buckets. We
find that doubling the batch size 6 times decreases the miss rate
tenfold, regardless the actual size or the number of buckets. In fact,
as the close parallel lines show (on the right in Figure~\ref{fig-mean}), there is no interaction:
going from one bucket to a billion increases the miss rate
only by tenfold, independently of bucket size.


\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.49\textwidth]{fig/fig-mean-1.pdf} \includegraphics[width=0.49\textwidth]{fig/fig-mean-2.pdf}
  \caption[Expected miss rate, for various parameters.]{\label{fig-mean}Expected miss rate, for various parameters. Left: miss rate against the number of buckets. Right: miss rate against bucket size.}
\end{figure}


\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.49\textwidth]{fig/fig-sd-1.pdf} \includegraphics[width=0.49\textwidth]{fig/fig-sd-2.pdf}
  \caption[Normalized standard deviation of utilization rate]{\label{fig-sd}Normalized standard deviation of utilization rate, for various parameters. Left: relative standard deviation against the number of buckets. Right: relative standard deviation against bucket size.}
\end{figure}




Looking at relative standard deviation of the distribution for various values of $k$ and $n$ (see Figure~\ref{fig-sd}), we find not much variance whenever $k$ and $n$ are large. Nevertheless, for the sake of correctness, we check the quantile function of our extreme value distribution (see Figures~\ref{fig-qua} and \ref{fig-q}).

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.49\textwidth]{fig/fig-qua-1.pdf} \includegraphics[width=0.49\textwidth]{fig/fig-qua-2.pdf}
  \caption[Quantiles of the utilization rate]{\label{fig-qua}Quantiles of the utilization rate, for
various parameters. Left: the quantile function of the utilization
rate. Right: mean, median, 10\%, and 1\% percentiles of utilization against the number of buckets.}
\end{figure}

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.49\textwidth]{fig/fig-q-1.pdf} \includegraphics[width=0.49\textwidth]{fig/fig-q-2.pdf}
  \caption[Quantiles of the miss rate]{Quantiles of the miss rate for various parameters. Left: one-in-a-thousand quantile for miss rate against bucket size. Right: miss rate in terms of percentiles, against number of buckets.}
\label{fig-q}
\end{figure}

 
Eventually, we aim to use the information on utilization rates to
protect the user from being surprised at the effective utilization of
their postage batch. To this end, we apply the volume calibration using
a reasonable worst-case scenario, i.e., define the effective utilization
rate as the one-in-a-thousand low-tail quantile of the utilization rate.
To obtain the ``effective'' volumes, we simply multiply the theoretical
purchased volume (given by $2^d$ where $d$ is the batch depth) by
the effective utilization rate.


Finally, in table~\ref{tab:batch-utilisation}, we provide these volume calibrations for $2^{12}$ and
$2^{16}$ buckets and log bucket sizes ranging from $4$ to $25$.
Above this bucket size the required quantile already shows a miss rate
below $0.1\%$. That is, there is only a one-in-a-thousand chance that
the difference between the effectively utilized volume and the
theoretically purchased one exceeds one tenth of a percent which we
consider insignificant enough to justify no calibration on the
theoretical volume.

\newpage

\begin{longtable}{rrrrrr}\toprule
\multicolumn{1}{c}{log2 no.}& 
\multicolumn{1}{c}{log2 size} & 
\multicolumn{1}{c}{log2 no.}&
\multicolumn{1}{c}{utilisation}&
\multicolumn{1}{c}{theoretical}&
\multicolumn{1}{c}{effective}\\
\multicolumn{1}{c}{of buckets}&
\multicolumn{1}{c}{of buckets}&
\multicolumn{1}{c}{of chunks}&
\multicolumn{1}{c}{rate}&
\multicolumn{1}{c}{volume}&
\multicolumn{1}{c}{volume}\\
\midrule
12 & 4 & 16 & 0.00000 & 268.44 MB & 0.00 B \\
12 & 5 & 17 & 0.06747 & 536.87 MB & 36.22 MB \\
12 & 6 & 18 & 0.34060 & 1.07 GB & 365.72 MB \\
12 & 7 & 19 & 0.53374 & 2.15 GB & 1.15 GB \\
12 & 8 & 20 & 0.67030 & 4.29 GB & 2.88 GB \\
12 & 9 & 21 & 0.76687 & 8.59 GB & 6.59 GB \\
12 & 10 & 22 & 0.83515 & 17.18 GB & 14.35 GB \\
12 & 11 & 23 & 0.88343 & 34.36 GB & 30.35 GB \\
12 & 12 & 24 & 0.91758 & 68.72 GB & 63.06 GB \\
12 & 13 & 25 & 0.94172 & 137.44 GB & 129.43 GB \\
12 & 14 & 26 & 0.95879 & 274.88 GB & 263.55 GB \\
12 & 15 & 27 & 0.97086 & 549.76 GB & 533.74 GB \\
12 & 16 & 28 & 0.97939 & 1.10 TB & 1.08 TB \\
12 & 17 & 29 & 0.98543 & 2.20 TB & 2.17 TB \\
12 & 18 & 30 & 0.98970 & 4.40 TB & 4.35 TB \\
12 & 19 & 31 & 0.99271 & 8.80 TB & 8.73 TB \\
12 & 20 & 32 & 0.99485 & 17.59 TB & 17.50 TB \\
12 & 21 & 33 & 0.99636 & 35.18 TB & 35.06 TB \\
12 & 22 & 34 & 0.99742 & 70.37 TB & 70.19 TB \\
12 & 23 & 35 & 0.99818 & 140.74 TB & 140.48 TB \\
12 & 24 & 36 & 0.99871 & 281.47 TB & 281.11 TB \\
12 & 25 & 37 & 0.99909 & 562.95 TB & 562.44 TB \\
16 & 4 & 20 & 0.00000 & 4.29 GB & 0.00 B \\
16 & 5 & 21 & 0.00000 & 8.59 GB & 0.00 B \\
16 & 6 & 22 & 0.28669 & 17.18 GB & 4.93 GB \\
16 & 7 & 23 & 0.49561 & 34.36 GB & 17.03 GB \\
16 & 8 & 24 & 0.64334 & 68.72 GB & 44.21 GB \\
16 & 9 & 25 & 0.74781 & 137.44 GB & 102.78 GB \\
16 & 10 & 26 & 0.82167 & 274.88 GB & 225.86 GB \\
16 & 11 & 27 & 0.87390 & 549.76 GB & 480.43 GB \\
16 & 12 & 28 & 0.91084 & 1.10 TB & 1.00 TB \\
16 & 13 & 29 & 0.93695 & 2.20 TB & 2.06 TB \\
16 & 14 & 30 & 0.95542 & 4.40 TB & 4.20 TB \\
16 & 15 & 31 & 0.96848 & 8.80 TB & 8.52 TB \\
16 & 16 & 32 & 0.97771 & 17.59 TB & 17.20 TB \\
16 & 17 & 33 & 0.98424 & 35.18 TB & 34.63 TB \\
16 & 18 & 34 & 0.98885 & 70.37 TB & 69.58 TB \\
16 & 19 & 35 & 0.99212 & 140.74 TB & 139.63 TB \\
16 & 20 & 36 & 0.99443 & 281.47 TB & 279.91 TB \\
16 & 21 & 37 & 0.99606 & 562.95 TB & 560.73 TB \\
16 & 22 & 38 & 0.99721 & 1.13 PB & 1.12 PB \\
16 & 23 & 39 & 0.99803 & 2.25 PB & 2.25 PB \\
16 & 24 & 40 & 0.99861 & 4.50 PB & 4.50 PB \\
16 & 25 & 41 & 0.99901 & 9.01 PB & 9.00 PB \\
\bottomrule
\caption{Theoretical and effective volumes of postage batches}
\label{tab:batch-utilisation}
\end{longtable}
